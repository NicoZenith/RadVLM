{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders for radiology datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from radvlm.data.utils import custom_collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "from radvlm.data.create_instructions import format_boxes\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create a function to display images with potentially some BBox. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def show_img(array_data, boxes=None):\n",
    "    # Display the image\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(array_data[0], cmap='gray')\n",
    "    \n",
    "    # Get image dimensions\n",
    "    img_height, img_width = array_data[0].shape\n",
    "\n",
    "    # Draw bounding boxes if provided\n",
    "    if boxes:\n",
    "        for box in boxes:\n",
    "            # Convert proportional coordinates to pixel coordinates\n",
    "            x1, y1, x2, y2 = box\n",
    "            x1_pixel = x1 * img_width\n",
    "            y1_pixel = y1 * img_height\n",
    "            x2_pixel = x2 * img_width\n",
    "            y2_pixel = y2 * img_height\n",
    "            \n",
    "            # Calculate width and height of the bounding box\n",
    "            width = x2_pixel - x1_pixel\n",
    "            height = y2_pixel - y1_pixel\n",
    "            \n",
    "            # Create a rectangle patch\n",
    "            rect = patches.Rectangle((x1_pixel, y1_pixel), width, height, linewidth=3, edgecolor='r', facecolor='none')\n",
    "            \n",
    "            # Add the rectangle to the plot\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    # Optional: Add colorbar and show the plot\n",
    "    plt.colorbar(ax.imshow(array_data[0], cmap='gray'), ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "def display_instruction(instruction):\n",
    "    return print(json.dumps(instruction, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's retrieve the env variable `DATA_DIR`, to indicate where all datasets are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radvlm import DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIMIC-CXR dataset - Report generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230980\n"
     ]
    }
   ],
   "source": [
    "from radvlm.data.datasets import MIMIC_Dataset_MM\n",
    "\n",
    "datasetpath = os.path.join(DATA_DIR, 'MIMIC-CXR-JPG')\n",
    "filtered_reports_dir = os.path.join(datasetpath, 'filtered_reports') # if you have the filtered reports dir\n",
    "conversation_dir =  os.path.join(datasetpath, 'conversations/train/standard') # if present \n",
    "\n",
    "dataset = MIMIC_Dataset_MM(\n",
    "    datasetpath=datasetpath,\n",
    "    split='train',\n",
    "    flag_img=True, # set to True if you want the get_item function to get the images\n",
    "    flag_lab=True, #  # set to True if you want the get_item function to get the labels\n",
    "    flag_instr=True, # set to True to create instructions for report generation\n",
    "    only_frontal=True, # set to True to ignore lateral images\n",
    "    filtered_reports_dir=filtered_reports_dir, # will show filtered reports. Set to None for original reports\n",
    "    sentencesBBoxpath=None, # indicate if you want to keep only the subset of samples from MS-CXR \n",
    "    conversation_dir=None, # indicate if you want to keep only the subset of samples that have conversations\n",
    ")\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a dataloader from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size = 1, shuffle = True, collate_fn=custom_collate_fn )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore some samples with images and attributes. If you want a new sample, simply reload the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(data_loader))\n",
    "\n",
    "print(\"\\n--------- Image Path ---------\")\n",
    "print(sample[0][\"img_path\"])\n",
    "\n",
    "print(\"\\n--------- Image ---------\")\n",
    "show_img(sample[0][\"img\"])\n",
    "\n",
    "print(\"\\n--------- Report ---------\")\n",
    "print(sample[0][\"txt\"])\n",
    "\n",
    "print(\"\\n--------- Labels ---------\")\n",
    "print(sample[0][\"labels\"])\n",
    "\n",
    "print(\"\\n--------- RG Instructions ---------\")\n",
    "display_instruction(sample[0][\"instr\"])\n",
    "\n",
    "#if you added conversation dir in the arguments \n",
    "# print(\"\\n--------- Conversation ---------\")\n",
    "# display_instruction(sample[0][\"conversation\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MS-CXR - Phrase Grounding\n",
    "This class is derived from MIMIC-CXR, and bring the grounded phrases. It is organized per phrase (different datapoints can have same image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964\n"
     ]
    }
   ],
   "source": [
    "from radvlm.data.datasets import MS_CXR\n",
    "\n",
    "datasetpath_mimic = os.path.join(DATA_DIR, 'MIMIC-CXR-JPG') # we need this to get the images as it is derived from MIMIC-CXR\n",
    "sentencesBBoxpath = os.path.join(DATA_DIR, 'MS-CXR','sentences_and_BBox_mscxr')\n",
    "dataset = MS_CXR(\n",
    "    datasetpath = datasetpath_mimic,\n",
    "    split=\"train\", \n",
    "    flag_img=True, \n",
    "    flag_lab=True, \n",
    "    only_frontal=True, \n",
    "    flag_instr=True, \n",
    "    sentencesBBoxpath=sentencesBBoxpath,\n",
    "    seed=0)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size = 1, shuffle = True, collate_fn=custom_collate_fn )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(data_loader))\n",
    "\n",
    "print(\"\\n--------- Image Path ---------\")\n",
    "print(sample[0][\"img_path\"])\n",
    "\n",
    "print(\"\\n--------- Phrase ---------\")\n",
    "print(sample[0][\"label\"])\n",
    "\n",
    "print(\"\\n--------- Bounding box ---------\")\n",
    "print(format_boxes(sample[0][\"boxes\"]))\n",
    "\n",
    "print(\"\\n--------- Image ---------\")\n",
    "show_img(sample[0][\"img\"], sample[0][\"boxes\"])\n",
    "\n",
    "print(\"\\n--------- Phrase grounding Instruction ---------\")\n",
    "display_instruction(sample[0][\"instr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chest ImaGenome dataset - Anatomical grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162794\n"
     ]
    }
   ],
   "source": [
    "from radvlm.data.datasets import Chest_ImaGenome_Dataset\n",
    "\n",
    "datasetpath_mimic = os.path.join(DATA_DIR, 'MIMIC-CXR-JPG') # we need this to get the images\n",
    "datasetpath_chestima = os.path.join(DATA_DIR, 'CHEST_IMA')\n",
    "filtered_reports_dir = os.path.join(datasetpath_mimic, 'filtered_reports') # if you have the filtered reports dir\n",
    "\n",
    "split = \"train\"\n",
    "dataset = Chest_ImaGenome_Dataset(\n",
    "    datasetpath=datasetpath_mimic,\n",
    "    datasetpath_chestima=datasetpath_chestima, \n",
    "    split=split, \n",
    "    filtered_reports_dir=None, # optional, if you want filtered reports llm-generated\n",
    "    flag_img=True, \n",
    "    flag_instr=True, \n",
    "    flag_txt=True, \n",
    "    flag_lab=False,\n",
    "    pick_one_region=True, # if you want just one (randomly picked) region to be retrieved in the get_item. Set to False to if you want them all \n",
    "    sentencesBBoxpath=None,\n",
    "    )\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size = 1, shuffle = True, collate_fn=custom_collate_fn)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(data_loader))\n",
    "\n",
    "print(\"\\n--------- Image Path ---------\")\n",
    "print(sample[0][\"img_path\"])\n",
    "\n",
    "print(\"\\n--------- Report ---------\")\n",
    "print(sample[0][\"txt\"])\n",
    "\n",
    "print(\"\\n--------- Region name ---------\")\n",
    "print(sample[0][\"label\"])\n",
    "\n",
    "print(\"\\n--------- Bounding box ---------\")\n",
    "print(format_boxes(sample[0][\"boxes\"])) # formating to round to 2 floating point numbers, just for display purposes\n",
    "\n",
    "print(\"\\n--------- Image ---------\")\n",
    "show_img(sample[0][\"img\"], sample[0][\"boxes\"])\n",
    "\n",
    "print(\"\\n--------- Anatomy grounding Instructions ---------\")\n",
    "display_instruction(sample[0][\"instr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VinDr-CXR dataset - Abnormality detection\n",
    "This dataset class is designed for abnormality detection: it contains all samples from the original VinDr-CXR (healthy and non healthy), and displays instructions for abnormliaty detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    }
   ],
   "source": [
    "from radvlm.data.datasets import VinDr_CXR_Dataset\n",
    "\n",
    "datasetpath = os.path.join(DATA_DIR, \"VinDr-CXR\") \n",
    "\n",
    "dataset = VinDr_CXR_Dataset(\n",
    "    datasetpath=datasetpath, \n",
    "    split=\"train\", \n",
    "    flag_img=True, \n",
    "    flag_instr=True,\n",
    "    )\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size = 1, shuffle = True, collate_fn=custom_collate_fn, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(data_loader))\n",
    "\n",
    "\n",
    "print(\"\\n--------- Image Path ---------\")\n",
    "print(sample[0][\"img_path\"])\n",
    "\n",
    "print(\"\\n--------- Abnormality names ---------\")\n",
    "print(sample[0][\"labels\"])\n",
    "\n",
    "print(\"\\n--------- Bounding box ---------\")\n",
    "print(sample[0][\"boxes\"])\n",
    "\n",
    "print(\"\\n--------- Image ---------\")\n",
    "show_img(sample[0][\"img\"], sample[0][\"boxes\"])\n",
    "\n",
    "print(\"\\n--------- Abnormality detection instruction ---------\")\n",
    "display_instruction(sample[0][\"instr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VinDr-CXR for mono-class grounding\n",
    "This class is designed for abnormality grounding: there are no healthy samples, only samples that contain abnormality; plus, there is only one abnormality per sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16089\n"
     ]
    }
   ],
   "source": [
    "from radvlm.data.datasets import VinDr_CXR_Single_Label_Dataset\n",
    "\n",
    "\n",
    "datasetpath = os.path.join(DATA_DIR, \"VinDr-CXR\") \n",
    "\n",
    "dataset = VinDr_CXR_Single_Label_Dataset(\n",
    "    datasetpath=datasetpath, \n",
    "    split=\"train\", \n",
    "    flag_img=True, \n",
    "    flag_instr=True,\n",
    "    )\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size = 1, shuffle = True, collate_fn=custom_collate_fn, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(data_loader))\n",
    "\n",
    "print(\"\\n--------- Image Path ---------\")\n",
    "print(sample[0][\"img_path\"])\n",
    "\n",
    "print(\"\\n--------- Abnormality name ---------\")\n",
    "print(sample[0][\"label\"])\n",
    "\n",
    "print(\"\\n--------- Bounding box ---------\")\n",
    "print(format_boxes(sample[0][\"boxes\"]))\n",
    "\n",
    "print(\"\\n--------- Image ---------\")\n",
    "show_img(sample[0][\"img\"], sample[0][\"boxes\"])\n",
    "\n",
    "print(\"\\n--------- Abnormality grounding instruction} ---------\")\n",
    "display_instruction(sample[0][\"instr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CheXpert Dataset - abnormality classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radvlm.data.datasets import CheXpert_Dataset_MM\n",
    "\n",
    "datasetpath = os.path.join(DATA_DIR, 'CheXpert')\n",
    "dataset = CheXpert_Dataset_MM(\n",
    "    datasetpath=datasetpath,\n",
    "    split='train',\n",
    "    unique_patients=False, \n",
    "    only_frontal=True, \n",
    "    flag_img=True,\n",
    "    flag_instr=True,\n",
    "    flag_lab=True,\n",
    ")\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size = 1, shuffle = True, collate_fn=custom_collate_fn, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(data_loader))\n",
    "\n",
    "print(\"\\n--------- Image Path ---------\")\n",
    "print(sample[0][\"img_path\"])\n",
    "\n",
    "print(\"\\n--------- Image ---------\")\n",
    "show_img(sample[0][\"img\"])\n",
    "\n",
    "print(\"\\n--------- Abnormality labels ---------\")\n",
    "print(sample[0][\"labels\"])\n",
    "\n",
    "print(\"\\n--------- Abnormality classification instruction} ---------\")\n",
    "display_instruction(sample[0][\"instr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CheXpert-Plus - report generation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186463\n"
     ]
    }
   ],
   "source": [
    "from radvlm.data.datasets import CheXpertPlus_Dataset\n",
    "\n",
    "datasetpath = os.path.join(DATA_DIR, 'CheXpert')\n",
    "filtered_reports_dir = os.path.join(datasetpath, 'filtered_reports')\n",
    "dataset = CheXpertPlus_Dataset(\n",
    "    datasetpath=datasetpath, \n",
    "    split='train',\n",
    "    flag_img=True,\n",
    "    flag_txt=True, \n",
    "    flag_lab=True,\n",
    "    only_frontal=True, \n",
    "    filtered_reports_dir=filtered_reports_dir, # optional, set to None for original reports\n",
    ")\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size = 1, shuffle = True, collate_fn=custom_collate_fn, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(data_loader))\n",
    "\n",
    "print(\"\\n--------- Image Path ---------\")\n",
    "print(sample[0][\"img_path\"])\n",
    "\n",
    "print(\"\\n--------- Image ---------\")\n",
    "show_img(sample[0][\"img\"])\n",
    "\n",
    "print(\"\\n--------- Report ---------\")\n",
    "print(sample[0][\"txt\"])\n",
    "\n",
    "print(\"\\n--------- Labels ---------\")\n",
    "print(sample[0][\"labels\"])\n",
    "\n",
    "print(\"\\n--------- RG Instructions ---------\")\n",
    "display_instruction(sample[0][\"instr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PadChest - Phrase grounding\n",
    "This dataset class is organized per observations, i.e., different datapoints can contain the same image (like for VinDr-CXR mono class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radvlm.data.datasets import PadChest_grounding\n",
    "\n",
    "datasetpath = os.path.join(DATA_DIR, 'PadChest')\n",
    "dataset = PadChest_grounding(\n",
    "    datasetpath=datasetpath,\n",
    "    split='train',\n",
    "    flag_instr=True,\n",
    "    flag_img=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size = 1, shuffle = True, collate_fn=custom_collate_fn, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(data_loader))\n",
    "\n",
    "print(\"\\n--------- Image Path ---------\")\n",
    "print(sample[0][\"img_path\"])\n",
    "\n",
    "print(\"\\n--------- Image ---------\")\n",
    "show_img(sample[0][\"img\"])\n",
    "\n",
    "print(\"\\n--------- Report ---------\")\n",
    "print(sample[0][\"txt\"])\n",
    "\n",
    "print(\"\\n--------- Phrase ---------\")\n",
    "print(sample[0][\"label\"])\n",
    "\n",
    "print(\"\\n--------- RG Instructions ---------\")\n",
    "display_instruction(sample[0][\"instr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to load the dataset per image with conversations, use the other class (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1945\n"
     ]
    }
   ],
   "source": [
    "from radvlm.data.datasets import PadChest_grounding_per_image\n",
    "\n",
    "conversation_dir = os.path.join(datasetpath, 'conversations/train/grounding')\n",
    "\n",
    "dataset = PadChest_grounding_per_image(\n",
    "    datasetpath=datasetpath,\n",
    "    split='train',\n",
    "    flag_img=True, \n",
    "    conversation_dir=conversation_dir\n",
    ")\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size = 1, shuffle = True, collate_fn=custom_collate_fn, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(data_loader))\n",
    "\n",
    "print(\"\\n--------- Image Path ---------\")\n",
    "print(sample[0][\"img_path\"])\n",
    "\n",
    "print(\"\\n--------- Image ---------\")\n",
    "show_img(sample[0][\"img\"])\n",
    "\n",
    "print(\"\\n--------- Report ---------\")\n",
    "print(sample[0][\"txt\"])\n",
    "\n",
    "print(\"\\n--------- Phrases ---------\")\n",
    "display_instruction(sample[0][\"sentencesBBox\"])\n",
    "\n",
    "print(\"\\n--------- Grounded conversations ---------\")\n",
    "display_instruction(sample[0][\"conversation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
